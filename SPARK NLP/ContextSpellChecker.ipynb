{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mdyAEezkca",
        "outputId": "8acc70ed-1be0-49dc-f34f-b2184d030dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.5/579.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import array_contains\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import (\n",
        "    Tokenizer,\n",
        "    ContextSpellCheckerModel,\n",
        "    ContextSpellCheckerApproach,\n",
        "    SentenceDetector,\n",
        "    NorvigSweetingModel,\n",
        "    SymmetricDeleteModel,\n",
        ")\n",
        "from sparknlp.common import RegexRule\n",
        "from sparknlp.base import DocumentAssembler, LightPipeline"
      ],
      "metadata": {
        "id": "Fy8WgO2lzuK4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5hI2cGnupxP",
        "outputId": "2dd4cf70-5a7c-4343-e212-6982e73112f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  5.4.2\n",
            "Apache Spark version:  3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"Plaese alliow me tao introdduce myhelf, I am a man of waelht und tiaste\""
      ],
      "metadata": {
        "id": "sOBMK7JIvLLz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_light_pipeline(spellModel):\n",
        "  documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "  tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "  pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellModel])\n",
        "\n",
        "  empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "  lp = LightPipeline(pipeline.fit(empty_ds))\n",
        "  return lp\n",
        "\n"
      ],
      "metadata": {
        "id": "TttnIIiz2rPh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "\n",
        "spellModel = (\n",
        "    ContextSpellCheckerModel.pretrained(\"spellcheck_dl\")\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    )\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellModel])\n",
        "\n",
        "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "lp = LightPipeline(pipeline.fit(empty_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3BY4gkt4u4o",
        "outputId": "0a8c7338-299a-4403-e6c2-bab56543bc4d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 95.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spellModel.getWordClasses()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MosFaeJ4z15",
        "outputId": "ed535b40-eb18-48ea-8c2f-83e3b4725c16"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(_NAME_,VocabParser)',\n",
              " '(_DATE_,RegexParser)',\n",
              " '(_LOC_,VocabParser)',\n",
              " '(_NUM_,RegexParser)']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spellModel.setOutputCol(\"checked\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiAsEZJk7GRL",
        "outputId": "bc0e7ffa-2743-4bfe-f611-1ff9f207df86"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SPELL_eaf90fb024f0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spellModel.getMaxWindowLen()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc9lPf1-7PMM",
        "outputId": "f07f8993-2434-44b8-cb05-7aee525e996a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spellModel.getCaseStrategy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6OHWK_97WFG",
        "outputId": "9c9e344c-aa65-4ccf-dc57-0ba1e3ddf3ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spellModel.getWordMaxDistance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrHaz61g-RIc",
        "outputId": "dc757b7d-cf9d-40c1-c069-5d0bf8195032"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = lp.annotate(example_sentence)\n",
        "print(result[\"checked\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckE45LnO7cBm",
        "outputId": "0c1aeff9-e9c3-4184-82b0-a99a0e7b283c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Please', 'allow', 'me', 'tao', 'introduce', 'myself', ',', 'I', 'am', 'a', 'man', 'of', 'waelht', 'und', 'taste']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jIcnUGo8i3d",
        "outputId": "7d940f4c-9523-4515-9852-dab136cc30b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'document': ['Plaese alliow me tao introdduce myhelf, I am a man of waelht und tiaste'], 'token': ['Plaese', 'alliow', 'me', 'tao', 'introdduce', 'myhelf', ',', 'I', 'am', 'a', 'man', 'of', 'waelht', 'und', 'tiaste'], 'checked': ['Please', 'allow', 'me', 'tao', 'introduce', 'myself', ',', 'I', 'am', 'a', 'man', 'of', 'waelht', 'und', 'taste']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, checked in zip(result[\"token\"], result[\"checked\"]):\n",
        "  print(f\"{token} -> {checked}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZyz7RsJ8rJ1",
        "outputId": "af40900d-50e4-4fe7-84c2-bae217b50e32"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plaese -> Please\n",
            "alliow -> allow\n",
            "me -> me\n",
            "tao -> tao\n",
            "introdduce -> introduce\n",
            "myhelf -> myself\n",
            ", -> ,\n",
            "I -> I\n",
            "am -> am\n",
            "a -> a\n",
            "man -> man\n",
            "of -> of\n",
            "waelht -> waelht\n",
            "und -> und\n",
            "tiaste -> taste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spellModel_modified = (\n",
        "    ContextSpellCheckerModel.pretrained(\"spellcheck_dl\")\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    .setWordMaxDistance(1)\n",
        ")\n",
        "\n",
        "lp = get_light_pipeline(spellModel_modified)\n",
        "result = lp.annotate(example_sentence)\n",
        "\n",
        "for token, checked in zip(result[\"token\"], result[\"checked\"]):\n",
        "  print(f\"{token} -> {checked}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05w3qGPN9I5z",
        "outputId": "635108c9-7d7c-4f4b-c250-bd1197f26255"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 95.1 MB\n",
            "[OK!]\n",
            "Plaese -> Please\n",
            "alliow -> allow\n",
            "me -> me\n",
            "tao -> tao\n",
            "introdduce -> introduce\n",
            "myhelf -> myself\n",
            ", -> ,\n",
            "I -> I\n",
            "am -> am\n",
            "a -> a\n",
            "man -> man\n",
            "of -> of\n",
            "waelht -> waelht\n",
            "und -> und\n",
            "tiaste -> taste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a context-aware spell checker"
      ],
      "metadata": {
        "id": "N--ahp0bBHhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/holmes.txt\n"
      ],
      "metadata": {
        "id": "bOCwYPRJ-txw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"holmes.txt\"\n",
        "\n",
        "corpus = spark.read.text(path).toDF(\"text\")\n",
        "corpus.show(truncate=800)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QAYTQruBT6E",
        "outputId": "96df839c-4259-4ae8-8272-e924e22061b7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|THE ADVENTURES OF SHERLOCK HOLMESArthur Conan Doyle Table of contents A Scandal in Bohemia The Red-Headed League A Case of Identity The Boscombe Valley Mystery The Five Orange Pips The Man with the Twisted Lip The Adventure of the Blue Carbuncle The Adventure of the Speckled Band The Adventure of the Engineer's Thumb The Adventure of the Noble Bachelor The Adventure of the Beryl Coronet The Adventure of the Copper Beeches A SCANDAL IN BOHEMIA Table of contents Chapter 1 Chapter 2 Chapter 3CHAPTER I To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to hi...|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "\n",
        "spellChecker = (\n",
        "    ContextSpellCheckerApproach()\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    .setBatchSize(1)\n",
        "    .setEpochs(1)\n",
        "    .setWordMaxDistance(3)\n",
        "    .setMaxWindowLen(3)\n",
        "    .setMinCount(3.0)\n",
        "    .setCompoundCount(3)\n",
        "    .setClassCount(5)\n",
        "\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellChecker])\n"
      ],
      "metadata": {
        "id": "DZr2l_a_Dro2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        " model = pipeline.fit(corpus)\n",
        "\n",
        "except Exception as e:\n",
        " print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUcKBXkFQQF",
        "outputId": "b1411728-bf51-4ae5-891a-fd85c69aa173"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirement failed: We couldn't find any suitable graph for 2000 classes, vocabSize: 3094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the corpus for training\n",
        "We will use the SentenceDetector annotator to split the book into sentences. Then we will sample a number of sentences that Colab is able to process. As a deep learning model, it demands heavy computation during training. For big datasets, it is recommended to use spark clusters to train efficiently."
      ],
      "metadata": {
        "id": "TcryYGyAKm5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
        "\n",
        "sentences = sentenceDetector.transform(documentAssembler.transform(corpus))\n",
        "\n",
        "# Get 10% of the senteces only\n",
        "sample = sentences.select(F.explode(\"sentence.result\").alias(\"sentence\")).sample(fraction=0.1, seed=42)\n",
        "sample.count()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuHLAfFCIK5A",
        "outputId": "33d244f9-3228-4df2-a3f6-69840da18364"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "561"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "Create a new pipeline to process this sample from the beginning (DocumentAssembler -> ContextSpellChecker)"
      ],
      "metadata": {
        "id": "2QNyUQbVMxJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Note that we se `sentence` as input column name\n",
        "documentAssembler = DocumentAssembler().setInputCol(\"sentence\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "\n",
        "spellChecker = (\n",
        "    ContextSpellCheckerApproach()\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    .setBatchSize(1) # Batch size 1 to run in Colab\n",
        "    .setEpochs(1)\n",
        "    .setWordMaxDistance(3) # Maximum edit distance to consider\n",
        "    .setMaxWindowLen(3) # important to find context\n",
        "    .setMinCount(3.0) # Removes words that appear less than that from the vocabulary\n",
        "    .setCompoundCount(5) # Removes compound words that appear less than that from the vocabulary\n",
        "    .setClassCount(10.0) # Minimun occurrences of a class\n",
        "    .setLanguageModelClasses(1650) # Value taht we have a TF graph available\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellChecker])"
      ],
      "metadata": {
        "id": "OEv_ZEw9Lnck"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "try:\n",
        "  model = pipeline.fit(sample)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNvxUvidL3iL",
        "outputId": "3b6a2abe-d04a-47d8-f758-dd48a087db27"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 263 ms, sys: 24.9 ms, total: 288 ms\n",
            "Wall time: 49.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lp = LightPipeline(model)\n",
        "\n",
        "test = lp.annotate(\"Sherlok Hlmes founds the solution to the mistrey\")\n",
        "\n",
        "for token, checked in zip(test[\"token\"], test[\"checked\"]):\n",
        "  print(f\"{token} => {checked}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5IFUaunNBTI",
        "outputId": "5bc4dca1-344c-43df-c8bb-24d4e6dc0233"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sherlok => Sherlock\n",
            "Hlmes => Holmes\n",
            "founds => found\n",
            "the => the\n",
            "solution => solution\n",
            "to => to\n",
            "the => the\n",
            "mistrey => mystery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iiJpBM9NzS-"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}